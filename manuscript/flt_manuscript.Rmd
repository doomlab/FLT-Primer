---
title             : "A practical primer on processing semantic property norm data"
shorttitle        : "Processing Norms"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "326 Market St., Harrisburg, PA 17101"
    email         : "ebuchanan@harrisburgu.edu"
  - name          : "Simon De Deyne"
    affiliation   : "2"
  - name          : "Maria Montefinese"
    affiliation   : "3"    

affiliation:
  - id            : "1"
    institution   : "Harrisburg University of Science and Technology"
  - id            : "2"
    institution   : "University of Melbourne"
  - id            : "3"
    institution   : "University of Padua"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Semantic property listing tasks require participants to generate short propositions (e.g., \<barks\>, \<has fur\>) for a specific concept (e.g., dog). This task is the cornerstone of the creation of semantic property norms which are essential for modelling, stimuli creation, and understanding similarity between concepts. However, despite the wide applicability of semantic property norms for a large variety of concepts across different groups of people, the methodological aspects of the property listing task have received less attention, even though the procedure and processing of the data can substantially affect the nature and quality of the measures derived from them. The goal of this paper is to provide a practical primer on how to collect and process semantic property norms. We will discuss the key methods to elicit semantic properties and compare different methods to derive meaningful representations from them.  This will cover the role of instructions and test context, property pre-processing (e.g., lemmatization), property weighting, and relationship encoding using ontologies. With these choices in mind, we propose and demonstrate a processing pipeline that transparently documents these steps resulting in improved comparability across different studies. The impact of these choices will be demonstrated using intrinsic (e.g. reliability, number of properties) and extrinsic measures (e.g., categorization, semantic similarity, lexical processing). Example data and the impact of choice decisions will be provided. This practical primer will offer potential solutions to several longstanding problems and allow researchers to develop new property listing norms overcoming the constraints of previous studies.
  
keywords          : "semantic, property norm task, tutorial"

bibliography      : ["flt_bib.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf #or _docx for word
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
library("papaja")
library(kableExtra)
library(cowplot)
library(ggplot2)
cleanup = theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line.x = element_line(colour = "black"), 
              axis.line.y = element_line(colour = "black"),
              legend.key = element_rect(fill = "white"),
              text = element_text(size = 12),
              axis.text.x = element_text(size = 12),
              axis.text.y = element_text(size = 12))

def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

1.	Available feature norms and their format

- Property listing task original work: @Toglia1978; @Toglia2009; @Rosch1975; @Ashcraft1978a 
- English: @McRae2005, @Vinson2008, @Buchanan2013, @Devereux2014, @Buchanan2019
- Italian: @Montefinese2013; @Reverberi2004, @Kremer2011a
- German: @Kremer2011a
- Portuguese: @Stein2009
- Spanish: @Vivas2017
- Dutch: @Ruts2004
- Blind participants: @Lenci2013

I'm sure there are more, here's what we cited recently.

Define concept, feature for clarity throughout - make sure you use these two terms consistently. 

2.	Pointers about how to collect the data
a.	instructions, generation, verification, importance

I really like the way the CSLB did it: https://cslb.psychol.cam.ac.uk/propnorms

They showed the concept, then had a drop down menu for is/has/does, and then the participant typed in a final window. That type of system would solve about half the problems I am going to describe below about using multi-word sequences. Might be some other suggestions, but for that type of processing, you could do combinations and have more consistent data easily. 

3.	Typical operations performed on features

```{r flowchart, echo=FALSE, fig.cap ="Flow chart of proposed semantic processing feature steps.", fig.height=3, fig.width=4}
knitr::include_graphics("flow_chart.png")
```

In the next several sections, we provide a tutorial using *R* on how data from the semantic property norm task might be processed from raw input to finalized output. Figure \@ref(fig:flowchart) portrays the proposed set of steps including spell checking, lemmatization, exclusion of stop words, and final processing in a multi-word sequence approach or a bag of words approach. After detailing these steps, the final data form will compared to previous norms to determine the usefulness of this approach. 

## Materials and Data Format

```{r data, include = F}
## Importing the data in the provided data folder
master <- read.csv("../data/tidy_words.csv", stringsAsFactors = F)
## Show data structure
## Data is in tidy format with concept in the word column
## Participant answer in the answer column
head(master)
```

```{r tab1, echo = F, results = 'asis'}
kable(head(master), "latex", booktabs = T, row.names = F, 
      caption = "Example of Data Formatted for Tidy Data") %>% 
  column_spec(2, width = "30em")
```

The data for this tutorial includes `r nrow(master)` unique concept-feature responses for `r length(unique(master$word))` concepts from @Buchanan2019 that were included in @McRae2005, @Vinson2008, and @Bruni2014. The data should be structured in tidy format wherein each concept-feature observation is a row and each column is a variable [@Wickham2014]. Therefore, the data includes a `word` column with the normed concept and an `answer` column with the participant answer, as shown in Table \@ref(tab:tab1). 

This data was collected using the instructions provided by @McRae2005, however, in contrast to the suggestions for consistency detailed above [@Devereux2014], each participant was simply given a large text box to include their answer. Each answer includes multiple embedded features, and the tutorial proceeds to demonstrate potential processing addressing the data in this nature. With structured data entry for participants, the suggested processing steps are reduced. 

## Spelling 

Spell checking can be automated with the `hunspell` package in *R* [@Ooms2018], which is the spell checking library used in popular programs such as FireFox, Chrome, RStudio, and OpenOffice. Each `answer` can be checked for misspellings across an entire column of answers, which is located in the `master` dataset. The default dictionary is American English, and the `hunspell` vignettes provide details on how to import your own dictionary for non-English languages. The choice of dictionary should also normalize between multiple varieties of the same language, for example, the `"en_GB"` would convert to British English spellings. 

```{r check_spelling, echo = T, size="scriptsize", messages = F}
## Install the hunspell package if necessary
#install.packages("hunspell")
library(hunspell)
## Check the participant answers
## The output is a list of spelling errors for each line
spelling_errors <- hunspell(master$answer, dict = dictionary("en_US"))
```

The result from the `hunspell()` function is a list object of spelling errors for each row of data. For example, when responding to *`r master$word[175]`*, a participant wrote *`r trimws(gsub("  ", " ", master$answer[175]))`*, and the spelling errors were denoted as *`r paste0(spelling_errors[[175]], collapse = " ")`*. After checking for errors, the `hunspell_suggest()` function was used to determine the most likely replacement for each error.

```{r get_suggestions, echo = T, size="scriptsize"}
## Check for suggestions
spelling_suggest <- lapply(spelling_errors, hunspell_suggest)
```

For *`r spelling_errors[[175]][1]`*, both *`r paste0(unlist(spelling_suggest[[175]][1]), collapse = " ")`* were suggested, and *`r paste0(unlist(spelling_suggest[[175]][2]), collapse = " ")`* were suggested for *`r spelling_errors[[175]][2]`*. The suggestions are presented in most probable order, and using a few loops with the substitute (`gsub()`) function, we can replace all errors with the most likely replacement in a new dataset `spell_checked`. A specialized dictionary with precoded error responses and corrections could be implemented at this stage. Other paid alternatives, such as Bing Spell Check, can be a useful avenue for datasets that may contain brand names (i.e, *apple* versus *Apple*) or slang terms. 

```{r fix_errors, echo = T, size="scriptsize"}
## Replace with most likely suggestion
spell_checked <- master
### Loop over the dataframe
for (i in 1:nrow(spell_checked)){
  ### See if there are spelling errors
  if (length(spelling_errors[[i]]) > 0) {
    ### Loop over all errors
    for (q in 1:length(spelling_errors[[i]])){
      ### Replace with the first answer
      spell_checked$answer[i] <- gsub(spelling_errors[[i]][q], 
                                      spelling_suggest[[i]][[q]][1],
                                      spell_checked$answer[i])
    }
  }
}
```

## Lemmatization

The next step approaches the clustering of word forms into their lemma or head word from a dictionary. The process of lemmatizing words involves using a lexeme set (i.e., all words forms that have the same meaning, *am, are, is*) to convert into a common lemma (i.e., *be*) from a trained dictionary. In contrast, stemming involves processing words using heuristics to remove affixes or inflections, such as *ing* or *s*. The stem or root word may not reflect an actual word in the langauge, as simply removing an affix does not necessarily produce the lemma. For example, in response to *airplane*, *flying* can be easily converted to *fly* by removing the *ing* inflection. However, this same heuristic converts the feature *wings* into *w* after removing both the *s* for a plural marker and the *ing* participle marker. Several packages for *R* include customizable stemmers, notably the `hunspell`, `corpus` [@Perry2017], and `tm` [@Feinerer2018] packages.

Lemmatization is the likely choice for processing property norms, and this process can be achieved by installing `TreeTagger` [@Schmid1994] and the `koRpus` package in *R* [@Michalke2018]. TreeTagger is a trained tagger designed to annotate part of speech and lemma information in text, and parameter files are available for multiple langauges. The koRpus package includes functionality to use TreeTagger in *R*. After installing the package and TreeTagger, we will create a unique set of tokenized words to lemmatize to speed computation. 

```{r lemma, echo = T, size="scriptsize", messages = F}
lemmas <- spell_checked
## Install the koRpus package
#install.packages("koRpus")
#install.packages("koRpus.lang.en")
## You must load both packages separately
library(koRpus)
library(koRpus.lang.en)
## Install TreeTagger 
#https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/
## Find all types for faster lookup
all_answers <- tokenize(lemmas$answer, format = "obj", tag = F)
all_answers <- unique(all_answers)
```

The `treetag()` function calls the installation of TreeTagger to provide part of speech tags and lemmas for each token. Importantly, the `path` option should be the directory of the TreeTagger installation. 

```{r lemma_treetag, echo = T, size="scriptsize"}
## This function has both suppressWarnings & suppressMessages
## You should first view these to ensure proper processing
temp_tag <- suppressWarnings(
  suppressMessages(
    ## Note: the NULL option is to control for the <unknown> that appears
    ## to occur with the last word in each text
    treetag(c(all_answers, "NULL"), 
            ## Control the parameters of treetagger
            treetagger="manual", format="obj",
            TT.tknz=FALSE, lang="en",
            TT.options=list(path="~/TreeTagger", preset="en"))))
```

This function returns a tagged corpus object, which can be converted into a dataframe of the token-lemma information. The goal would be to replace inflected words with their lemmas, and therefore, unknown values, number tags, and equivalent values are ignored by subseting out these from the dataset. Table \@ref(tab:tab2) portrays the results from TreeTagger.  

```{r lemma_remove, echo = T, size="scriptsize"}
## Remove all tags not using
replacement_lemmas <- temp_tag@TT.res
replacement_lemmas <- subset(replacement_lemmas, 
                             #ignore punctuation
                             wclass != "punctuation" &
                             #unknown values
                             lemma != "<unknown>" & 
                             #numbers
                             lemma!= "@card@" & 
                             #token should change more than case
                             tolower(token) != tolower(lemma)) 
```

```{r tab2, echo = F, results = 'asis'}
kable(head(replacement_lemmas[ , 2:6]), "latex", booktabs = T, row.names = F, 
      caption = "Lemma and Part of Speech Information from TreeTagger")
```

From this dataset, you can use the `stringi` package [@Gagolewski2019] to replace all of the original tokens with their lemmas. This package allows for replacement lookup across a large set of subsitutions. The `stri_replace_all_regex()` function includes the column of data to examine, the patterns to find (using `\\b` regular expressions to ensure word boundaries and no partial word replacements), what to replace those patterns with, and other options to ensure the original dataframe with replacement is returned. Table \@ref(tab:tab3) shows the processed data at this stage. 

```{r string_replace, echo = T, size="scriptsize"}
## Install the stringi package
#install.packages("stringi")
library(stringi)
## Replace all the original tokens with new lemmas using \\b for word boundaries
lemmas$answer <- stri_replace_all_regex(str = lemmas$answer, 
                       pattern = paste("\\b", replacement_lemmas$token, "\\b", sep = ""),
                       replacement = replacement_lemmas$lemma,
                       vectorize_all = F, list(case_insensitive = TRUE))
```

```{r tab3, echo = F, results = 'asis'}
kable(head(lemmas), "latex", booktabs = T, row.names = F, 
      caption = "Original Data with Lemmatization") %>%  
  column_spec(2, width = "30em")
```

## Word Sequences

Multi-word sequences are often coded to mimic a @Collins1969 style model, with "is-a" and "has-a" type markers. If data were collected to include these markers, this step would be pre-encoded into the output data, rendering the following code unnecessary. A potential solution for processing messy data could be to search for specific part of speech sequences that mimic the "is-a" and "has-a" strings. An examination of the coding in @McRae2005 and @Devereux2014 indicates that the feature tags are often verb-noun or verb-adjective-noun sequences. Using TreeTagger on each concept's answer set, we can obtain the parts of speech in context for each lemma. With `dplyr` [@Wickham2019], new columns are added to tagged data to show all bigram and trigram sequences. All verb-noun and verb-adjective-noun combinations are selected, and any words not part of these multi-word sequences are treated as unigrams. Finally, the `table()` function is used to tabulate the final count of n-grams and their frequency. 

```{r multi_words, echo = T, size="scriptsize", message = F}
## Create an empty dataframe 
multi_words <- data.frame(Word=character(),
                        Feature=character(), 
                        Frequency=numeric(), 
                        stringsAsFactors=FALSE) 
## Create unique word list to loop over 
unique_concepts <- unique(lemmas$word)
## Install dplyr
#install.packages("dplyr")
library(dplyr)
## Loop over each word
for (i in 1:length(unique_concepts)){
  ## Create parts of speech for clustering together
  temp_tag <- suppressWarnings(
    suppressMessages(
      treetag(c(lemmas$answer[lemmas$word  == unique_concepts[i]], "NULL"), 
          ## Control the parameters of treetagger
          treetagger="manual", format="obj",
          TT.tknz=FALSE, lang="en",
          TT.options=list(path="~/TreeTagger", preset="en"))))
  ## Save only the dataframe, remove NULL
  temp_tag <- temp_tag@TT.res[-nrow(temp_tag@TT.res) , ]
  ## Subset out information you don't need
  temp_tag <- subset(temp_tag, 
                     wclass != "comma" & wclass != "determiner" & 
                       wclass != "preposition" & wclass != "modal" &
                       wclass != "predeterminer" & wclass != "particle" &
                       wclass != "to" & wclass != "punctuation" & 
                       wclass != "fullstop" & wclass != "conjunction" & 
                       wclass != "pronoun")
  ## Create a temporary tibble 
  temp_tag_tibble <- as_tibble(temp_tag)
  ## Create part of speech and features combined
  temp_tag_tibble <- mutate(temp_tag_tibble, 
                            two_words = paste(token, 
                                              lead(token), sep = "_"))
  temp_tag_tibble <- mutate(temp_tag_tibble, 
                            three_words = paste(token, 
                                                lead(token), lead(token, n = 2L), 
                                                sep = "_"))
  temp_tag_tibble <- mutate(temp_tag_tibble, 
                            two_words_pos = paste(wclass, 
                                                  lead(wclass), sep = "_"))
  temp_tag_tibble <- mutate(temp_tag_tibble, 
                            three_words_pos = paste(wclass, 
                                                    lead(wclass), lead(wclass, n = 2L), 
                                                    sep = "_"))
  ## Find verb noun or verb adjective nouns to cluster on 
  verb_nouns <- grep("\\bverb_noun", temp_tag_tibble$two_words_pos)
  verb_adj_nouns <- grep("\\bverb_adjective_noun", temp_tag_tibble$three_words_pos)
  ## Use combined and left over features
  features_for_table <- c(temp_tag_tibble$two_words[verb_nouns], 
                          temp_tag_tibble$three_words[verb_adj_nouns],
                          temp_tag_tibble$token[-c(verb_nouns, verb_nouns+1, 
                                                   verb_adj_nouns, verb_adj_nouns+1, 
                                                   verb_adj_nouns+2)])
  ## Create a table of frequencies
  word_table <- as.data.frame(table(features_for_table))
  ## Clean up the table
  word_table$Word <- unique_concepts[i]
  colnames(word_table) = c("Feature", "Frequency", "Word")
  multi_words <- rbind(multi_words, word_table[ , c(3, 1, 2)])
}
```

This procedure produces mostly positive output, such as *fingers-have_fingernails* and *couches-have_cushions*. One obvious limitation is the potential necessity to match this coding system to previous codes, which were predominately hand processed. Further, many similar phrases, such as the ones for *zebra* shown below may require fuzzy logic matching to ensure that the different codings for *is-a-horse* are all combined together, as shown in Table \@ref(tab:tab4).

```{r tab4, echo = F, results = 'asis'}
zebra <- multi_words[ grep("horse", multi_words$Feature) , ]
zebra <- zebra[zebra$Word == "zebra", ]

kable(zebra[1:10, ], "latex", booktabs = T, row.names = F, 
      caption = "Multi-Word Sequence Examples for Zebra") 
```

## Bag of Words 

The bag of words approach simply treats each token as a separate feature to be tabulated for analysis. After stemming and lemmatization, the data can be processed as single word tokens into a table of frequencies for each cue word. The resulting dataframe is each cue-feature combination with a total for each feature.

```{r bag_words, echo = T, size="scriptsize", message = F}
## Create an empty dataframe 
bag_words <- data.frame(Word=character(),
                        Feature=character(), 
                        Frequency=numeric(), 
                        stringsAsFactors=FALSE) 
## Loop over each word
for (i in 1:length(unique_concepts)){
  ## Create a table of frequencies
  word_table <- as.data.frame(table(
    ## Tokenize the words
    tokenize(
      ## Put all answers together in one character string
      paste0(lemmas$answer[lemmas$word == unique_concepts[i]], collapse = " "), 
      format = "obj", tag = F)))
  
  ## Clean up the table
  word_table$Word <- unique_concepts[i]
  colnames(word_table) = c("Feature", "Frequency", "Word")
  
  bag_words <- rbind(bag_words, word_table[ , c(3, 1, 2)])
}
## Remove punctuation
bag_words <- bag_words[-c(grep('^[[:punct:]]',bag_words$Feature)), ]
```

Tab \@ref(tab:tab5) shows the top ten most frequent responses to *zebra* given the bag of words approach. The top ten features in zebra indicate a match to the multi-word sequence approach but the inclusion of words such as *be, in, a* indicate the need to remove irrelevant words listed with features.  

```{r tab5, echo = F, results = 'asis'}
zebra <- bag_words[ bag_words$Word == "zebra" , ]
zebra <- zebra[order(zebra$Frequency, decreasing = T), ]

kable(zebra[1:10, ], "latex", booktabs = T, row.names = F, 
      caption = "Bag of Words Examples for Zebra") 
```

## Stopwords

As shown in Figure \@ref(fig:flowchart), the next stage of processing would be to exclude stopwords, such as *the, of, but*, for either the multi-word sequence or bag of word style processing. The `stopwords` package [@Benoit2017] includes a list of stopwords for more than 50 languages. For multi-word sequence processing, these values can be removed by subseting the data to exclude stopwords as unigrams. 

``` {r stop_words_multi, echo = T, size="scriptsize"}
## Install the stopwords package or use tm
#install.packages("stopwords")
library(stopwords)
## Remove stop words from either processing approach
multi_words <- subset(multi_words, 
                      !(Feature %in% stopwords(language = "en", 
                                               source = "snowball")))

bag_words <- subset(bag_words, 
                    !(Feature %in% stopwords(language = "en", 
                                             source = "snowball")))
```

## Descriptive Statistics

The finalized data now represents a a processed set of cue-feature combinations with their frequencies for analysis. Given the differences in sample size across data collection points from @Buchanan2019, this information was merged with the sample data. Table \@ref(tab:tab6) includes descriptive statistics for the processed cue-feature set. First, the number of cue-feature combinations was calculated by taking the average number of cue-feature listings for each cue. Therefore, the total number of features listed for *zebra* might be 100, while *apple* might be 45, and these values were averaged. 

More cue-feature combinations are listed for the multi-word approach, likely due to differences in combinations for some overlapping features as shown in Table \@ref(tab:tab4). The large standard deviation for both approaches indicates that cues have a wide range of possible features listed. The correlation provided represents the relation between sample size for a cue and the number of features listed for that cue. These values are high and positive, indicating that the number of unique features increases with each participant. Potentially, many of the cue-feature combinations could be considered idiosyncratic. The next row of the table denotes the average number of cue-feature responses listed by less than 10% of the participants. This percent of responses is somewhat arbitrary, as each researcher has determined where the optimal criterion should be. For example, @McRae2005 used 16% or 5/30 participants as a minimum standard, and @Buchanan2019 recently used a similar criteria. The average number of cue-features that would be considered low in proportion is quite large, indicating that these are potentially idiosyncratic or part of long tailed distribution of feature responses with many low frequency features. The advantage to the suggested data processing pipeline and code provided here is the ability of each researcher to determine their own level of response necessary, if desired. 

The next two lines of Table \@ref(tab:tab6) indicate cue-feature combination frequecies, such as the number of times *zebra-stripes* or *apple-red* were listed by participants. The percent of responses is the frequency divided by sample size for each cue, to normalize over different sample sizes present in the data. These average frequency/percent was calculated for each cue, and then averaged over all cues. The correlation represents the average frequency/percent for each cue related to the sample size for that cue. These frequencies are low, matching the results for a large number of idiosyncratic responses. The correlation between frequency of response and sample size is positive, indicating that larger sample sizes produce items with larger frequencies. Additionally, the correlation between percent of response and sample size is negative, suggesting that larger sample sizes are often paired with more items with smaller percent likelihoods. Figure \@ref(fig:correlation-fig) displays the correlations for the average cue-frequency responses and the percent cue-frequency responses by sample size. It appears that the relationship between sample size and percent is likely curvilinear, rather than linear. The size of the points indicates the variability (standard deviation of each cue word's average frequency or percent). Variability appears to increase linearly with sample size for average frequency, however, it is somewhat mixed for average percent.  

```{r desc_stats, echo = F, include = F}
sample_sizes <- as.data.frame(tapply(master$word, master$word, length))
sample_sizes$Word <- rownames(sample_sizes)
colnames(sample_sizes)[1] <- "Sample_Size"

multi_words <- merge(multi_words, sample_sizes, by = "Word")
bag_words <- merge(bag_words, sample_sizes, by = "Word")

#number of features
no_feat_m <- tapply(multi_words$Frequency, multi_words$Word, length)
no_feat_b <- tapply(bag_words$Frequency, bag_words$Word, length)

#correlations
cor(no_feat_b, no_feat_m)
cor(no_feat_m, sample_sizes$Sample_Size)
cor(no_feat_b, sample_sizes$Sample_Size)

#number of responses
no_resp_mM <- tapply(multi_words$Frequency, multi_words$Word, mean)
no_resp_bM <- tapply(bag_words$Frequency, bag_words$Word, mean)
no_resp_mSD <- tapply(multi_words$Frequency, multi_words$Word, sd)
no_resp_bSD <- tapply(bag_words$Frequency, bag_words$Word, sd)

multi_words$Percent <- multi_words$Frequency/multi_words$Sample_Size*100
bag_words$Percent <- bag_words$Frequency/bag_words$Sample_Size*100

#number of responses proportionate to sample size
per_resp_mM <- tapply(multi_words$Percent, multi_words$Word, mean)
per_resp_bM <- tapply(bag_words$Percent, bag_words$Word, mean)
per_resp_mSD <- tapply(multi_words$Percent, multi_words$Word, sd)
per_resp_bSD <- tapply(bag_words$Percent, bag_words$Word, sd)

#correlations
cor(no_resp_mM, no_resp_bM)
cor(no_resp_mM, sample_sizes$Sample_Size)
cor(no_resp_bM, sample_sizes$Sample_Size)

#number of small features
no_smallfeat_m <- tapply(multi_words$Percent[multi_words$Percent <=10], multi_words$Word[multi_words$Percent <=10], length)
no_smallfeat_b <- tapply(bag_words$Percent[bag_words$Percent <=10], bag_words$Word[bag_words$Percent <=10], length)
```

```{r tab6, echo = F, results = 'asis'}
tableprint <- matrix(NA, nrow = 4, ncol = 7)

colnames(tableprint) <- c("Stat", "M_m", "SD_m", "r_m", 
                          "M_b", "SD_b", "r_b")

tableprint <- as.data.frame(tableprint)
#number of cue-features
tableprint[1, ] <- c("Number of Cue-Features", 
                     mean(no_feat_m), sd(no_feat_m), cor(no_feat_m, sample_sizes$Sample_Size), 
                     mean(no_feat_b), sd(no_feat_b), cor(no_feat_b, sample_sizes$Sample_Size))

#avg small features
tableprint[2, ] <- c("Frequency of Idiosyncratic Response", 
                     mean(no_smallfeat_m), sd(no_smallfeat_m), 
                     cor(no_smallfeat_m, sample_sizes$Sample_Size),
                     mean(no_smallfeat_b), sd(no_smallfeat_b),
                     cor(no_smallfeat_b, sample_sizes$Sample_Size))

#avg no times cue-feature listed
tableprint[3, ] <- c("Frequency of Cue-Feature Response", 
                     mean(no_resp_mM), mean(no_resp_mSD), 
                     cor(no_resp_mM, sample_sizes$Sample_Size),
                     mean(no_resp_bM), mean(no_resp_bSD), 
                     cor(no_resp_bM, sample_sizes$Sample_Size))

#avg percent cue-feature listed
tableprint[4, ] <- c("Percent of Cue-Feature Response", 
                     mean(per_resp_mM), mean(per_resp_mSD), 
                     cor(per_resp_mM, sample_sizes$Sample_Size),
                     mean(per_resp_bM), mean(no_resp_bSD),
                     cor(per_resp_bM, sample_sizes$Sample_Size))

for (i in 2:7){
  tableprint[ , i] <- printnum(as.numeric(tableprint[ , i]))
}

kable(tableprint, "latex", booktabs = T, row.names = F,
      escape = F, 
      col.names = c("Statistics", "$M$", "$SD$", "$r$", 
                          "$M$", "$SD$", "$r$"), 
      caption = "Descriptive Statistics of Text Processing Style") %>%  
  add_header_above(c(" " = 1, "Multi-Word Sequences" = 3, "Bag of Words" = 3)) %>% 
  add_footnote("$Note$. Correlation represents the relation between the statistic listed for that row and the sample size for the cue.", notation="none", escape = F)
```

```{r correlation-fig, echo = F, fig.height=6, fig.width=8, fig.cap="Correlation of sample size with the average cue-feature frequency (left) and percent (right) of response for each cue for both processing approaches. Each point represents a cue word, and the size of the point indicates the variability of the average frequency or percent."}

p1 <- ggplot(data = NULL, aes(no_resp_bM, sample_sizes$Sample_Size)) +
  geom_point(alpha = .25, size = no_resp_bSD) +
  xlab("Average Frequency Cue-Feature Response") +
  ylab("Cue Sample Size") + 
  labs(title="Bag of Words") +
  cleanup

p2 <- ggplot(data = NULL, aes(per_resp_bM, sample_sizes$Sample_Size)) +
  geom_point(alpha = .25, size = per_resp_bSD) +
  xlab("Average Percent Cue-Feature Response") +
  ylab("Cue Sample Size") + 
  labs(title="Bag of Words") +
  cleanup

p3 <- ggplot(data = NULL, aes(no_resp_mM, sample_sizes$Sample_Size)) +
  geom_point(alpha = .25, size = no_resp_mSD) +
  xlab("Average Frequency Cue-Feature Response") +
  ylab("Cue Sample Size") + 
  labs(title="Multi-word Sequences") +
  cleanup

p4 <- ggplot(data = NULL, aes(per_resp_mM, sample_sizes$Sample_Size)) +
  geom_point(alpha = .25, size = per_resp_mSD) +
  xlab("Average Percent Cue-Feature Response") +
  ylab("Cue Sample Size") + 
  labs(title="Multi-word Sequences") +
  cleanup

prow <- plot_grid(p1, p2, p3, p4, hjust = -1, nrow = 2)
prow
```

## Internal Comparison of Approach

In this section, we show that the bag of words approach processed completely through code matches a bag of words approach that was hand coded from @Buchanan2019. In @Buchanan2019, the @McRae2005 and @Vinson2008 datasets were recoded in a bag of words approach, and the comparison between all three is provided below. The mutli-word sequence approach would be comparable if one or more datasets used the same structured data collection approach or with considerable hand coded rules for feature combinations. The data from open ended responses, such as the @Buchanan2019, could potentially be compared in the demonstrated multi-word sequence approach, if the raw data from other such projects were avaliable. 

```{r compare_b, echo = F, include = F}
library(readxl)
b_data <- read.csv("../data/final words 2017.csv", stringsAsFactors = F)

## Reduce down only to overlap words
b_data <- subset(b_data, cue %in% unique_concepts)
b_data <- b_data[ , c("where", "cue", "feature", "translated",
                      "frequency_feature", "frequency_translated") ]

library(lsa)

cosine_values <- data.frame(word=unique_concepts,
                            raw_b=1:length(unique_concepts), 
                            raw_m=1:length(unique_concepts),
                            raw_v=1:length(unique_concepts),
                            translated_b=1:length(unique_concepts), 
                            translated_m=1:length(unique_concepts), 
                            translated_v=1:length(unique_concepts), 
                            stringsAsFactors=FALSE)

for (i in 1:length(unique_concepts)){
 
  temp_b <- b_data[b_data$cue == unique_concepts[i] & b_data$where == "b", ]
  temp_m <- b_data[b_data$cue == unique_concepts[i] & b_data$where == "m", ] 
  temp_v <- b_data[b_data$cue == unique_concepts[i] & b_data$where == "v", ]
  temp_bag <- bag_words[bag_words$Word == unique_concepts[i], ]
  colnames(temp_bag)[2] = "feature"
  
  ## Based on unprocessed data 
  temp_merge_b <- merge(temp_b, temp_bag, by = "feature", all = T)
  temp_merge_m <- merge(temp_m, temp_bag, by = "feature", all = T)
  temp_merge_v <- merge(temp_v, temp_bag, by = "feature", all = T)
  
  temp_merge_b[c("frequency_feature", "Frequency")][is.na(temp_merge_b[c("frequency_feature", "Frequency")])] <- 0
  temp_merge_m[c("frequency_feature", "Frequency")][is.na(temp_merge_m[c("frequency_feature", "Frequency")])] <- 0
  temp_merge_v[c("frequency_feature", "Frequency")][is.na(temp_merge_v[c("frequency_feature", "Frequency")])] <- 0
  
  cosine_values$raw_b[i] <- cosine(temp_merge_b$frequency_feature, temp_merge_b$Frequency)
  cosine_values$raw_m[i] <- cosine(temp_merge_m$frequency_feature, temp_merge_m$Frequency)
  cosine_values$raw_v[i] <- cosine(temp_merge_v$frequency_feature, temp_merge_v$Frequency)
  
  ## Based on processed data
  colnames(temp_bag)[2] = "translated"
  temp_merge_b <- merge(temp_b, temp_bag, by = "translated", all = T)
  temp_merge_m <- merge(temp_m, temp_bag, by = "translated", all = T)
  temp_merge_v <- merge(temp_v, temp_bag, by = "translated", all = T)
  
  temp_merge_b[c("frequency_translated", "Frequency")][is.na(temp_merge_b[c("frequency_translated", "Frequency")])] <- 0
  temp_merge_m[c("frequency_translated", "Frequency")][is.na(temp_merge_m[c("frequency_translated", "Frequency")])] <- 0
  temp_merge_v[c("frequency_translated", "Frequency")][is.na(temp_merge_v[c("frequency_translated", "Frequency")])] <- 0
  
  ### This process creates duplicates
  temp_merge_b <- temp_merge_b[!duplicated(temp_merge_b$translated), ]
  temp_merge_m <- temp_merge_m[!duplicated(temp_merge_m$translated), ]
  temp_merge_v <- temp_merge_v[!duplicated(temp_merge_v$translated), ]
  
  cosine_values$translated_b[i] <- cosine(temp_merge_b$frequency_translated, temp_merge_b$Frequency)
  cosine_values$translated_m[i] <- cosine(temp_merge_m$frequency_translated, temp_merge_m$Frequency)
  cosine_values$translated_v[i] <- cosine(temp_merge_v$frequency_translated, temp_merge_v$Frequency)
  
  }

cosine_values$raw_m[is.nan(cosine_values$raw_m)] <- NA
cosine_values$raw_v[is.nan(cosine_values$raw_v)] <- NA
cosine_values$translated_m[is.nan(cosine_values$translated_m)] <- NA
cosine_values$translated_v[is.nan(cosine_values$translated_v)] <- NA

cosine_M <- apply(cosine_values[ , -1], 2, mean, na.rm = T)
cosine_SD <- apply(cosine_values[ , -1], 2, sd, na.rm = T)
cosine_N <- apply(cosine_values[ , -1], 2, function(x) length(na.omit(x)))
```

Cosine is often used as a measure of semantic similiarity, indicating the feature overlap between two sets of cue-feature lists. These values can range from 0 (no overlap) to 1 (perfect overlap). There are two potential cosine values from the @Buchanan2019: the raw cosine, which included all features as listed without lemmatization or stemming, and the translated cosine, which included hand lemmatization processing. Each cue in the sample data for this project was compared to the corresponding cue in the @Buchanan2019. If data were processed in an identical fashion, the cosine values would be nearly 1 for @Buchanan2019 data or match the cosine values found for @McRae2005 and @Vinson2008 in the @Buchanan2019 results (original feature cosine = .54-.55, translated features = .66-.67). However, all previous datasets have been reduced by eliminating idiosyncratic features at various points, and therefore, we might expect that noise in this data to reduce the average cosine values. The cosine matches for original features averaged: $M_B$ = `r printnum(cosine_M['raw_b'], gt1 = F)` (*SD* = `r printnum(cosine_SD['raw_b'], gt1 = F)`, *N* = `r printnum(cosine_N['raw_b'])`); $M_M$ = `r printnum(cosine_M['raw_m'], gt1 = F)` (*SD* = `r printnum(cosine_SD['raw_m'], gt1 = F)`, *N* = `r printnum(cosine_N['raw_m'])`); $M_V$ = `r printnum(cosine_M['raw_v'], gt1 = F)` (*SD* = `r printnum(cosine_SD['raw_v'], gt1 = F)`, *N* = `r printnum(cosine_N['raw_v'])`). These values indicate a somewhat comparable set of data, with lower values for @McRae2005 than previous results. The cosine matches for translated features averaged: $M_B$ = `r printnum(cosine_M['translated_b'], gt1 = F)` (*SD* = `r printnum(cosine_SD['translated_b'], gt1 = F)`, *N* = `r printnum(cosine_N['translated_b'])`); $M_M$ = `r printnum(cosine_M['translated_m'], gt1 = F)` (*SD* = `r printnum(cosine_SD['translated_m'], gt1 = F)`, *N* = `r printnum(cosine_N['translated_m'])`); $M_V$ = `r printnum(cosine_M['translated_v'], gt1 = F)` (*SD* = `r printnum(cosine_SD['translated_v'], gt1 = F)`, *N* = `r printnum(cosine_N['translated_v'])`). Again, these values indicate that the data processed entirely in *R* produces a comparable set of results, albeit with added noise of small frequency features. 

## External Comparison of Approach

```{r compare_men, echo = F}
men_data <- read.table("../data/MEN_dataset_natural_form_full.txt",
                       quote="\"", comment.char="", stringsAsFactors=FALSE)

library(tidyr)
bag_words_spread <- spread(bag_words[ , -c(4,5)], key = Word, value = Frequency, fill = 0)
bag_words_cosine <- cosine(as.matrix(bag_words_spread[ , -1]))
bag_words_cosine <- as_tibble(bag_words_cosine)
bag_words_cosine$cue <- colnames(bag_words_cosine)
bag_words_cosine <- gather(bag_words_cosine, key = target, 
                           value = cosine, abstract:zebra)
bag_words_cosine$key <- paste(bag_words_cosine$cue, bag_words_cosine$target, sep = " ")
men_data$key <- paste(men_data$V1, men_data$V2)
colnames(men_data) <- c("cue", "target", "rating", "key")
  
men_merge = merge(men_data, bag_words_cosine, by = "key")

##correlation
cor_MEN <- cor.test(men_merge$rating, men_merge$cosine)
```

The MEN dataset [@Bruni2014] contains cue-cue pairs of English words rating for similarity by Amazon Mechanical Turk participants. In their rating task, participants were shown two cue-cue pairs and asked to select the more related pair of the two presented. Each pair was rated by 50 participants, and thus, a score of 50 indicates high relatedness, while a score of 0 indicates no relatedness. A range of relatedness values were selected from this dataset with overlapping cues from @Buchanan2019, and these values were compared to the cosine caluclated between cues using the bag of words method. The correlation between cosine on the processed data and the MEN ratings was `r apa_print(cor_MEN)$estimate`, *N* = `r cor_MEN$parameter+2`, indicating considerable agreement between raters and cosine values. 


## Ontology and Categorization

Generally, coding ontology is cumbersome on the researcher, as it is normally performed by hand using a coding schema. @Wu2009 developed a hierarchical taxonomy for coding categories as part of the feature listing task, that has been used in several projects, notably the @McRae2005. Examples of the categories include taxonomic (synonyms, subordinates), entity (internal components, behavior, spatial relations), situation (location, time), and  introspective properties (emotion, evaluation). Coding ontology may be best peformed systematically with look-up rules of previously decided upon factors, however, clustering analyses provide a potential avenue to explore categorizing features within the current dataset. One limitation to this method the sheer size of the idiosyncractic features as mentioned above, and thus, features smaller in number may be more difficult to group. 

```{r categorization, echo = F, include = F}
library(cluster)
library(expss)

category_combos <- list()
bag_words_reduce <- bag_words

for (i in 1:10){
  
  bag_words_dist <- dist(bag_words_spread[ , -1], method = "euclidean")
  bag_words_cluster <- hclust(bag_words_dist, method = "ward.D2")
  bag_words_category <- as.data.frame(cutree(bag_words_cluster, k = 10))
  bag_words_category$feature <- bag_words_spread$Feature
  colnames(bag_words_category)[1] <- "category"
  #bag_words_category$totals <- rowSums(bag_words_spread[ , -1])
  cat_temp <- as.data.frame(table(bag_words_category$category))
  keep <- cat_temp$Var1[cat_temp$Freq > 10 & cat_temp$Freq<500]
  category_combos[[i]] <- subset(bag_words_category, category %in% keep)
  
  #remove words and start over
  bag_words_reduce <- bag_words_reduce[!(bag_words_reduce$Feature %in% unique(category_combos[[i]]$feature)) , ]
  bag_words_spread <- spread(bag_words_reduce[ , -c(4,5)], key = Word, value = Frequency, fill = 0)

}


```

The `cluster` library [@Maechler2019] was used to analyze a hierarchical cluster analysis on the Euclidean distances between cue-feature frequencies. The @Wu2009 taxonomy has 37 potential category markers, and therefore, this number of clusters was examined during an iterative process to cluster the data. During the first pass of clustering the features, one large cluster appeared with approximately 93% of the data. The second category included 4% of the data, and all other categories were small in nature. However, these small categories appear to cluster in meaningful ways, such as *egg, meal, milk, day, important, morning, bacon, toast, biscuit*, and this category represented ... 





# Discussion

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
